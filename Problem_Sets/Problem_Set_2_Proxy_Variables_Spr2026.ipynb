{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed858e28",
   "metadata": {},
   "source": [
    "# Problem Set #2: Proxy Variable Regression\n",
    "## Ec 143, Spring 2026\n",
    "_Bryan S. Graham_\n",
    "January 2026 (last updated Jan 2026)\n",
    "\n",
    "**Due by 5PM on February 25th.** The GSI, Jinglin Yang (jinglin.yang@berkeley.edu), will handle the logistics of problem set collection.    \n",
    "\n",
    "Working with your classmates on the problem set is actively encouraged, but everyone needs to turn in their own Jupyter Notebook and any other accompanying materials.  You must list all study partners on your turned in problem set. If you used AI to assist you in any way, please briefly describe how you used it and which one.  \n",
    "\n",
    "This problem set provides empirical practice with proxy variable regression material discussed in lecture, as well as with the Bayesian bootstrap.\n",
    "#### Code citation:\n",
    "<br>\n",
    "Graham, Bryan S. (2023). \"Proxy Variable Regression: Python Jupyter Notebook,\" (Version 1.0) [Computer program]. Available at http://bryangraham.github.io/econometrics/ (Accessed 18 March 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e11280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500af151",
   "metadata": {},
   "source": [
    "In this problem set you will work with an extract from the [National Longitudinal Survey of Youth](https://www.nlsinfo.org/) (NLSY) 1979 cohort. Specifically you will explore how the earning of this cohort around the year 2000 varies with their years of completed schooling and other background variables. This cohort was approximately 40 years old in 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e34d436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where nlsy79extract.csv file is located\n",
    "workdir =  '/Users/bgraham/Dropbox/Teaching/Berkeley_Courses/Ec143/Ec143_Spring2026/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cffc48fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID_79</th>\n",
       "      <th>HHID_79</th>\n",
       "      <th>core_sample</th>\n",
       "      <th>sample_wgts</th>\n",
       "      <th>month_born</th>\n",
       "      <th>year_born</th>\n",
       "      <th>live_with_mom_at_14</th>\n",
       "      <th>live_with_dad_at_14</th>\n",
       "      <th>single_mom_at_14</th>\n",
       "      <th>usborn</th>\n",
       "      <th>...</th>\n",
       "      <th>weeks_worked_2001</th>\n",
       "      <th>weeks_worked_2003</th>\n",
       "      <th>weeks_worked_2005</th>\n",
       "      <th>weeks_worked_2007</th>\n",
       "      <th>weeks_worked_2009</th>\n",
       "      <th>weeks_worked_2011</th>\n",
       "      <th>NORTH_EAST_79</th>\n",
       "      <th>NORTH_CENTRAL_79</th>\n",
       "      <th>SOUTH_79</th>\n",
       "      <th>WEST_79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>602156.31</td>\n",
       "      <td>9</td>\n",
       "      <td>58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>816100.38</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>572996.38</td>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>604567.88</td>\n",
       "      <td>8</td>\n",
       "      <td>62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>764753.00</td>\n",
       "      <td>7</td>\n",
       "      <td>59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PID_79  HHID_79  core_sample  sample_wgts  month_born  year_born  \\\n",
       "0       1        1            1    602156.31           9         58   \n",
       "1       2        2            1    816100.38           1         59   \n",
       "2       3        3            1    572996.38           8         61   \n",
       "3       4        3            1    604567.88           8         62   \n",
       "4       5        5            1    764753.00           7         59   \n",
       "\n",
       "   live_with_mom_at_14  live_with_dad_at_14  single_mom_at_14  usborn  ...  \\\n",
       "0                  1.0                  1.0               0.0     1.0  ...   \n",
       "1                  1.0                  1.0               0.0     0.0  ...   \n",
       "2                  1.0                  0.0               0.0     1.0  ...   \n",
       "3                  1.0                  0.0               0.0     1.0  ...   \n",
       "4                  1.0                  1.0               0.0     1.0  ...   \n",
       "\n",
       "   weeks_worked_2001  weeks_worked_2003  weeks_worked_2005  weeks_worked_2007  \\\n",
       "0                NaN                NaN                NaN                NaN   \n",
       "1                0.0               18.0               52.0               52.0   \n",
       "2                0.0                NaN               43.0                0.0   \n",
       "3                NaN                NaN                NaN                NaN   \n",
       "4                NaN                NaN                NaN                NaN   \n",
       "\n",
       "   weeks_worked_2009  weeks_worked_2011  NORTH_EAST_79  NORTH_CENTRAL_79  \\\n",
       "0                NaN                NaN            1.0               0.0   \n",
       "1               52.0               52.0            1.0               0.0   \n",
       "2                NaN               52.0            1.0               0.0   \n",
       "3                NaN                NaN            1.0               0.0   \n",
       "4                NaN                NaN            1.0               0.0   \n",
       "\n",
       "   SOUTH_79  WEST_79  \n",
       "0       0.0      0.0  \n",
       "1       0.0      0.0  \n",
       "2       0.0      0.0  \n",
       "3       0.0      0.0  \n",
       "4       0.0      0.0  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in NLSY79 Extract as a pandas dataframe\n",
    "nlsy79 = pd.read_csv(workdir+'nlsy79extract.csv') # Reading .csv file as DataFrame\n",
    "\n",
    "# Hierarchical index: household, then individual; keep indices as columns too\n",
    "nlsy79.set_index(['HHID_79','PID_79'], drop=False)\n",
    "nlsy79.rename(columns = {'AFQT_Adj':'AFQT'}, inplace=True) # Renaming AFQT\n",
    "\n",
    "#Display the first few rows of the dataframe\n",
    "nlsy79.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31aad7",
   "metadata": {},
   "source": [
    "We will work with a subsample of respondents that belong to the core sample, are male and have complete cases for all required variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fff5be18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID_79</th>\n",
       "      <th>HHID_79</th>\n",
       "      <th>year_born</th>\n",
       "      <th>usborn</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>black</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>HGC_Age28</th>\n",
       "      <th>AFQT</th>\n",
       "      <th>live_with_mom_at_14</th>\n",
       "      <th>live_with_dad_at_14</th>\n",
       "      <th>HGC_FATH79r</th>\n",
       "      <th>HGC_MOTH79r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3435.000000</td>\n",
       "      <td>3435.000000</td>\n",
       "      <td>3435.000000</td>\n",
       "      <td>3435.000000</td>\n",
       "      <td>3435.000000</td>\n",
       "      <td>3435.000000</td>\n",
       "      <td>3435.000000</td>\n",
       "      <td>3435.000000</td>\n",
       "      <td>3435.000000</td>\n",
       "      <td>3435.000000</td>\n",
       "      <td>3435.000000</td>\n",
       "      <td>3435.000000</td>\n",
       "      <td>3435.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5077.071616</td>\n",
       "      <td>5069.208151</td>\n",
       "      <td>60.830277</td>\n",
       "      <td>0.932751</td>\n",
       "      <td>0.184279</td>\n",
       "      <td>0.268705</td>\n",
       "      <td>53082.003979</td>\n",
       "      <td>12.923726</td>\n",
       "      <td>43.913141</td>\n",
       "      <td>0.948180</td>\n",
       "      <td>0.786026</td>\n",
       "      <td>10.821252</td>\n",
       "      <td>10.937700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3258.080955</td>\n",
       "      <td>3252.758283</td>\n",
       "      <td>2.190681</td>\n",
       "      <td>0.250489</td>\n",
       "      <td>0.387768</td>\n",
       "      <td>0.443350</td>\n",
       "      <td>49435.239705</td>\n",
       "      <td>2.409577</td>\n",
       "      <td>29.835416</td>\n",
       "      <td>0.221695</td>\n",
       "      <td>0.410168</td>\n",
       "      <td>4.150287</td>\n",
       "      <td>3.309359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2392.500000</td>\n",
       "      <td>2391.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23553.124625</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>16.698000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4695.000000</td>\n",
       "      <td>4693.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42477.267750</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>40.937000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7589.500000</td>\n",
       "      <td>7571.500000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>66368.866500</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>69.541500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12517.000000</td>\n",
       "      <td>12514.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>315153.720000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             PID_79       HHID_79    year_born       usborn     hispanic  \\\n",
       "count   3435.000000   3435.000000  3435.000000  3435.000000  3435.000000   \n",
       "mean    5077.071616   5069.208151    60.830277     0.932751     0.184279   \n",
       "std     3258.080955   3252.758283     2.190681     0.250489     0.387768   \n",
       "min        6.000000      5.000000    57.000000     0.000000     0.000000   \n",
       "25%     2392.500000   2391.500000    59.000000     1.000000     0.000000   \n",
       "50%     4695.000000   4693.000000    61.000000     1.000000     0.000000   \n",
       "75%     7589.500000   7571.500000    63.000000     1.000000     0.000000   \n",
       "max    12517.000000  12514.000000    64.000000     1.000000     1.000000   \n",
       "\n",
       "             black       Earnings    HGC_Age28         AFQT  \\\n",
       "count  3435.000000    3435.000000  3435.000000  3435.000000   \n",
       "mean      0.268705   53082.003979    12.923726    43.913141   \n",
       "std       0.443350   49435.239705     2.409577    29.835416   \n",
       "min       0.000000       0.000000     3.000000     0.000000   \n",
       "25%       0.000000   23553.124625    12.000000    16.698000   \n",
       "50%       0.000000   42477.267750    12.000000    40.937000   \n",
       "75%       1.000000   66368.866500    14.000000    69.541500   \n",
       "max       1.000000  315153.720000    20.000000   100.000000   \n",
       "\n",
       "       live_with_mom_at_14  live_with_dad_at_14  HGC_FATH79r  HGC_MOTH79r  \n",
       "count          3435.000000          3435.000000  3435.000000  3435.000000  \n",
       "mean              0.948180             0.786026    10.821252    10.937700  \n",
       "std               0.221695             0.410168     4.150287     3.309359  \n",
       "min               0.000000             0.000000     0.000000     0.000000  \n",
       "25%               1.000000             1.000000     8.000000    10.000000  \n",
       "50%               1.000000             1.000000    12.000000    12.000000  \n",
       "75%               1.000000             1.000000    12.000000    12.000000  \n",
       "max               1.000000             1.000000    20.000000    20.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only retain non-black, non-hispanic, male NLSY79 respondents belonging to core sample\n",
    "nlsy79 = nlsy79[(nlsy79.core_sample != 0) & (nlsy79.male != 0)]\n",
    "\n",
    "# Calculate average earnings across the 1997, 1999, 2001 and 2003 calendar years\n",
    "# NOTE: This is an average over non-missing earnings values\n",
    "nlsy79['Earnings'] = nlsy79[[\"real_earnings_1997\", \"real_earnings_1999\", \\\n",
    "                             \"real_earnings_2001\", \"real_earnings_2003\"]].mean(axis=1)\n",
    "\n",
    "# Only retain complete cases of year of birth, earnings, schooling, AFQT and family background\n",
    "nlsy79 = nlsy79[[\"PID_79\", \"HHID_79\", \"year_born\", \"usborn\", \"hispanic\", \"black\", \\\n",
    "                 \"Earnings\", \"HGC_Age28\", \"AFQT\", \\\n",
    "                 \"live_with_mom_at_14\", \"live_with_dad_at_14\", \"HGC_FATH79r\", \"HGC_MOTH79r\"]] \n",
    "nlsy79 = nlsy79.dropna()\n",
    "\n",
    "# Summary statistics\n",
    "nlsy79.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e4627",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "1. Complete the least squares fit of log(Earnings) onto a constant and HGC_Age28. You may use Python's StatsModels OLS implementation for computation and standard error construction (use the cov_type='HC3' option for heteroscedastic robust standard errors).\n",
    "2. Compute the least squares fit of log(Earnings) onto a constant and HGC_Age28, usborn, hispanic, black, AFQT, live_with_mom_at_14, live_with_dad_at_14, HGC_FATH79r, \"HGC_MOTH79r.\n",
    "3. Compute the least squares fit of HGC_Age28 onto a constant usborn, hispanic, black, AFQT, live_with_mom_at_14, live_with_dad_at_14, HGC_FATH79r, \"HGC_MOTH79r.\n",
    "4. Using your regression output from questions 2 and 3 only, (re-)compute the coefficient on HGC_Age28 in question 1.\n",
    "5. Compute the fitted residuals, say _V_hat_, associated with the auxiliary regression calculated in question 4.\n",
    "6. Compute the least squares fit of log(Earnings) onto the residuals computed in question 5. \n",
    "7. Provide a narrative summary of your analysis referencing the results you developed in the pencil and paper portion of the problem set. **[5-8 paragraphs]**.\n",
    "8. Use the Bayesian Bootstrap to simulate draws from the posterior distribution for the coefficient on HGC_Age28 in the long regression calculated in question 3. Using at least 1000 posterior draws, summarize this posterior distribution via histogram. Compute the posterior mean and standard deviation. How do these quantities compare to the least square point estimate and standard error computed in questions 3?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
